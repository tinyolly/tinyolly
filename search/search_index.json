{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"The World's First Desktop Observability Platform"},{"location":"#introducing-tinyolly","title":"Introducing TinyOlly","text":"<p>Repository: https://github.com/tinyolly/tinyolly</p> <pre><code>git clone https://github.com/tinyolly/tinyolly\n</code></pre>"},{"location":"#why-tinyolly","title":"Why TinyOlly?","text":"<p>Why send telemetry to a cloud observability platform while coding? Why not have one on your desktop?</p> <p>TinyOlly is a lightweight OpenTelemetry-native observability platform built from scratch to visualize and correlate logs, metrics, and traces. No 3rd party observability tools - just Python (FastAPI), Redis, OpenAPI, and JavaScript.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Development-focused - Perfect your app's telemetry locally before production</li> <li>Full OpenTelemetry support - Native OTLP ingestion (gRPC &amp; HTTP)</li> <li>Pre-built Docker images - Deploy in ~30 seconds from Docker Hub</li> <li>Multi-architecture - Supports linux/amd64 and linux/arm64 (Apple Silicon)</li> <li>Trace correlation - Link logs, metrics, and traces automatically</li> <li>Metrics Explorer - Analyze cardinality, labels, and raw series data</li> <li>Service catalog - RED metrics (Rate, Errors, Duration) for all services</li> <li>Interactive service map - Visualize dependencies and call graphs</li> <li>OpenTelemetry Collector management - Remote configuration management via OpAMP protocol</li> <li>REST API - Programmatic access with OpenAPI documentation</li> <li>Zero vendor lock-in - Works with any OTel Collector distribution</li> </ul> <p>Local Development Only</p> <p>TinyOlly is not designed to compete with production observability platforms! It's for local development only and is not focused on infrastructure monitoring at this time.</p>"},{"location":"#platform-support","title":"Platform Support","text":"<p>Tested on:</p> <ul> <li>Docker Desktop (macOS Apple Silicon)</li> <li>Minikube Kubernetes (macOS Apple Silicon)</li> <li>May work on other platforms</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Ready to try TinyOlly? Check out the Quick Start Guide to get running in under 5 minutes!</p>"},{"location":"#screenshots","title":"Screenshots","text":"Trace Waterfall with Correlated Logs Real-time Logs with Filtering Metrics with Chart Visualization Service Catalog with RED Metrics Interactive Service Dependency Map OTel Collector Configuration (OpAMP) <p>Built for the OpenTelemetry community</p> <p> GitHub \u2022     Issues </p>"},{"location":"CARDINALITY-PROTECTION/","title":"Cardinality Protection","text":"<p>Span waterfall showing distributed trace complexity</p> <p>TinyOlly includes built-in protection against metric cardinality explosion with a configurable limit on unique metric names.</p>"},{"location":"CARDINALITY-PROTECTION/#configuration","title":"Configuration","text":""},{"location":"CARDINALITY-PROTECTION/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>MAX_METRIC_CARDINALITY</code> 1000 Maximum unique metric names <code>REDIS_TTL</code> 1800 Metric retention (seconds)"},{"location":"CARDINALITY-PROTECTION/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Update <code>k8s/tinyolly-otlp-receiver.yaml</code>:</p> <pre><code>env:\n  - name: MAX_METRIC_CARDINALITY\n    value: \"2000\"  # Increase limit\n  - name: REDIS_TTL\n    value: \"3600\"  # 1 hour retention\n</code></pre>"},{"location":"CARDINALITY-PROTECTION/#docker-deployment","title":"Docker Deployment","text":"<p>Update <code>docker-compose-tinyolly-core.yml</code> in the <code>docker/</code> directory:</p> <pre><code>environment:\n  MAX_METRIC_CARDINALITY: 2000\n  REDIS_TTL: 3600\n</code></pre>"},{"location":"CARDINALITY-PROTECTION/#monitoring","title":"Monitoring","text":"<p>The UI displays cardinality warnings when approaching the limit: - Yellow Warning: 70-90% of limit reached - Red Alert: 90%+ of limit reached</p> <p>Check current cardinality via the API:</p> <pre><code>curl http://localhost:5005/api/stats\n</code></pre> <p>Response: <pre><code>{\n  \"traces\": 145,\n  \"logs\": 892,\n  \"metrics\": 850,\n  \"metrics_max\": 1000,\n  \"metrics_dropped\": 23\n}\n</code></pre></p>"},{"location":"ai-agent/","title":"AI Agent Demo","text":"<p>This demo showcases GenAI observability with OpenTelemetry - automatic instrumentation of LLM calls using the <code>opentelemetry-instrumentation-ollama</code> package.</p>"},{"location":"ai-agent/#what-is-genai-observability","title":"What is GenAI Observability?","text":"<p>GenAI observability captures telemetry from LLM interactions including:</p> <ul> <li>Prompts and responses - Full text of user prompts and model outputs</li> <li>Token usage - Input and output token counts</li> <li>Latency - Response time for each LLM call</li> <li>Model information - Which model was used</li> </ul>"},{"location":"ai-agent/#quick-start","title":"Quick Start","text":""},{"location":"ai-agent/#docker","title":"Docker","text":"<pre><code># Start TinyOlly core first\ncd docker\n./01-start-core.sh\n\n# Deploy AI agent demo (pulls pre-built images from Docker Hub)\ncd ../docker-ai-agent-demo\n./01-deploy-ai-demo.sh\n</code></pre> <p>This starts:</p> <ul> <li>Ollama with TinyLlama model for local LLM inference</li> <li>AI Agent with automatic GenAI span instrumentation</li> </ul> <p>Access the UI at <code>http://localhost:5005</code> and navigate to the AI Agents tab.</p> <p>For local development: Use <code>./01-deploy-ai-demo-local.sh</code> to build locally</p> <p>Stop: <code>./02-stop-ai-demo.sh</code></p> <p>Cleanup (remove volumes): <code>./03-cleanup-ai-demo.sh</code></p>"},{"location":"ai-agent/#how-it-works","title":"How It Works","text":"<p>The demo uses zero-code auto-instrumentation - no OpenTelemetry imports in the application code:</p> <pre><code># agent.py - NO OpenTelemetry imports needed!\nfrom ollama import Client\n\nclient = Client(host=\"http://ollama:11434\")\n\n# This call is AUTO-INSTRUMENTED\nresponse = client.chat(\n    model=\"tinyllama\",\n    messages=[{\"role\": \"user\", \"content\": \"What is OpenTelemetry?\"}]\n)\n</code></pre> <p>The magic happens in the Dockerfile:</p> <pre><code># Install auto-instrumentation packages\nRUN pip install opentelemetry-distro opentelemetry-instrumentation-ollama\n\n# Run with auto-instrumentation wrapper\nCMD [\"opentelemetry-instrument\", \"python\", \"-u\", \"agent.py\"]\n</code></pre>"},{"location":"ai-agent/#what-youll-see","title":"What You'll See","text":"<p>In the AI Agents tab:</p> Field Description Prompt The user's input to the LLM Response The model's output Tokens In Number of input tokens Tokens Out Number of output tokens Latency Response time in milliseconds Model Model name (e.g., <code>tinyllama</code>) <p>Click any row to expand the full span details in JSON format.</p>"},{"location":"ai-agent/#supported-llms","title":"Supported LLMs","text":"<p>The OpenTelemetry GenAI semantic conventions work with any instrumented LLM provider:</p> <ul> <li>Ollama - Local LLM inference (this demo)</li> <li>OpenAI - GPT models via <code>opentelemetry-instrumentation-openai</code></li> <li>Anthropic - Claude models</li> <li>Other providers - Any with OpenTelemetry instrumentation</li> </ul>"},{"location":"ai-agent/#configuration","title":"Configuration","text":"<p>The demo is configured via environment variables in <code>docker-compose.yml</code>:</p> <pre><code>ai-agent:\n  environment:\n    - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317\n    - OTEL_SERVICE_NAME=ai-agent-demo\n    - OLLAMA_HOST=http://ollama:11434\n</code></pre>"},{"location":"ai-agent/#troubleshooting","title":"Troubleshooting","text":"<p>No AI traces appearing?</p> <ul> <li>Ensure TinyOlly core is running</li> <li>Check agent logs: <code>docker logs ai-agent-demo</code></li> <li>Verify Ollama is ready: <code>docker logs ollama</code></li> </ul> <p>Model download taking long?</p> <ul> <li>TinyLlama is ~600MB, first download may take a few minutes</li> <li>Check Ollama logs for download progress</li> </ul> <p>Agent errors?</p> <ul> <li>Ollama needs time to load the model after container starts</li> <li>The agent waits 10 seconds before first call</li> </ul>"},{"location":"api/","title":"REST API &amp; OpenAPI","text":"<p>OpenAPI documentation with interactive Swagger UI</p> <p>TinyOlly provides a comprehensive REST API for programmatic access to all telemetry data in OpenTelemetry-native format.</p>"},{"location":"api/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Access the auto-generated OpenAPI documentation: - Swagger UI: <code>http://localhost:5005/docs</code> - Interactive API explorer - ReDoc: <code>http://localhost:5005/redoc</code> - Alternative documentation - OpenAPI Spec: <code>http://localhost:5005/openapi.json</code> - Machine-readable schema</p> <p>All APIs return OpenTelemetry-native JSON with: - Resources: <code>service.name</code>, <code>host.name</code>, etc. - Attributes: Metric labels and span attributes - Full Context: Trace/span IDs, timestamps, status codes</p>"},{"location":"api/#api-endpoints-overview","title":"API Endpoints Overview","text":"<p>The REST API provides endpoints for:</p> Endpoint Method Description <code>/api/traces</code> GET List recent traces with filtering <code>/api/traces/{trace_id}</code> GET Get detailed trace with all spans <code>/api/spans</code> GET List recent spans with filtering <code>/api/logs</code> GET Retrieve logs with trace correlation <code>/api/metrics</code> GET Query time-series metrics <code>/api/service-map</code> GET Get service dependency graph <code>/api/service-catalog</code> GET List services with RED metrics <code>/api/stats</code> GET System stats and cardinality info <code>/admin/stats</code> GET Detailed admin statistics <code>/health</code> GET Health check endpoint <p>All endpoints return data in standard OpenTelemetry format, ensuring compatibility with OpenTelemetry tooling and standards.</p>"},{"location":"api/#common-api-workflows","title":"Common API Workflows","text":""},{"location":"api/#1-get-all-recent-traces","title":"1. Get All Recent Traces","text":"<p>Retrieve the last 50 traces:</p> cURLPythonJavaScript <pre><code>curl http://localhost:5005/api/traces?limit=50\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:5005/api/traces', params={'limit': 50})\ntraces = response.json()\n\nfor trace in traces:\n    print(f\"Trace {trace['trace_id']}: {trace['service_name']} - {trace['name']}\")\n</code></pre> <pre><code>fetch('http://localhost:5005/api/traces?limit=50')\n  .then(response =&gt; response.json())\n  .then(traces =&gt; {\n    traces.forEach(trace =&gt; {\n      console.log(`Trace ${trace.trace_id}: ${trace.service_name} - ${trace.name}`);\n    });\n  });\n</code></pre> <p>Response Format: <pre><code>[\n  {\n    \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n    \"service_name\": \"demo-frontend\",\n    \"name\": \"GET /products\",\n    \"start_time\": 1701234567890000000,\n    \"duration_ms\": 125.4,\n    \"status_code\": 200,\n    \"method\": \"GET\",\n    \"route\": \"/products\",\n    \"span_count\": 5\n  }\n]\n</code></pre></p>"},{"location":"api/#2-get-detailed-trace-with-waterfall","title":"2. Get Detailed Trace with Waterfall","text":"<p>Retrieve a complete trace with all spans for waterfall visualization:</p> cURLPython <pre><code>curl http://localhost:5005/api/traces/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\n</code></pre> <pre><code>import requests\n\ntrace_id = \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\nresponse = requests.get(f'http://localhost:5005/api/traces/{trace_id}')\ntrace = response.json()\n\nprint(f\"Trace: {trace['name']}\")\nprint(f\"Total spans: {len(trace['spans'])}\")\nprint(f\"Duration: {trace['duration_ms']}ms\")\n\nfor span in trace['spans']:\n    indent = \"  \" * span.get('level', 0)\n    print(f\"{indent}{span['name']} ({span['duration_ms']}ms)\")\n</code></pre> <p>Response Format: <pre><code>{\n  \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n  \"name\": \"GET /products\",\n  \"service_name\": \"demo-frontend\",\n  \"start_time\": 1701234567890000000,\n  \"duration_ms\": 125.4,\n  \"span_count\": 5,\n  \"spans\": [\n    {\n      \"span_id\": \"1234567890abcdef\",\n      \"parent_span_id\": null,\n      \"name\": \"GET /products\",\n      \"service_name\": \"demo-frontend\",\n      \"start_time\": 1701234567890000000,\n      \"duration_ms\": 125.4,\n      \"status\": {\"code\": 1},\n      \"attributes\": {\n        \"http.method\": \"GET\",\n        \"http.route\": \"/products\",\n        \"http.status_code\": 200\n      }\n    }\n  ]\n}\n</code></pre></p>"},{"location":"api/#3-find-logs-for-a-specific-trace","title":"3. Find Logs for a Specific Trace","text":"<p>Correlate logs with a trace using trace_id:</p> cURLPython <pre><code>curl \"http://localhost:5005/api/logs?trace_id=a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n</code></pre> <pre><code>import requests\n\ntrace_id = \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\nresponse = requests.get('http://localhost:5005/api/logs',\n                       params={'trace_id': trace_id})\nlogs = response.json()\n\nfor log in logs:\n    print(f\"[{log['severity']}] {log['body']}\")\n</code></pre> <p>Response Format: <pre><code>[\n  {\n    \"timestamp\": 1701234567890000000,\n    \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n    \"span_id\": \"1234567890abcdef\",\n    \"severity\": \"INFO\",\n    \"body\": \"Processing product request\",\n    \"service_name\": \"demo-frontend\",\n    \"attributes\": {\n      \"user_id\": \"12345\"\n    }\n  }\n]\n</code></pre></p>"},{"location":"api/#4-query-metrics","title":"4. Query Metrics","text":"<p>Retrieve metrics data:</p> cURLPython <pre><code>curl http://localhost:5005/api/metrics\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:5005/api/metrics')\nmetrics = response.json()\n\nfor metric in metrics:\n    print(f\"{metric['name']} ({metric['type']})\")\n    for series in metric.get('series', []):\n        labels = ', '.join(f\"{k}={v}\" for k, v in series.get('attributes', {}).items())\n        print(f\"  [{labels}] = {series.get('value', 'N/A')}\")\n</code></pre> <p>Response Format: <pre><code>[\n  {\n    \"name\": \"http.server.duration\",\n    \"type\": \"histogram\",\n    \"description\": \"HTTP request duration\",\n    \"unit\": \"ms\",\n    \"series\": [\n      {\n        \"attributes\": {\n          \"http.method\": \"GET\",\n          \"http.route\": \"/products\",\n          \"service.name\": \"demo-frontend\"\n        },\n        \"data_points\": [\n          {\n            \"timestamp\": 1701234567890000000,\n            \"count\": 42,\n            \"sum\": 5250.5,\n            \"bucket_counts\": [10, 20, 10, 2]\n          }\n        ]\n      }\n    ]\n  }\n]\n</code></pre></p>"},{"location":"api/#5-get-service-catalog-with-red-metrics","title":"5. Get Service Catalog with RED Metrics","text":"<p>List all services with Rate, Errors, and Duration metrics:</p> cURLPython <pre><code>curl http://localhost:5005/api/service-catalog\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:5005/api/service-catalog')\nservices = response.json()\n\nfor service in services:\n    print(f\"\\n{service['service_name']}\")\n    print(f\"  Request Rate: {service.get('request_rate', 0):.2f} req/s\")\n    print(f\"  Error Rate: {service.get('error_rate', 0):.2f}%\")\n    print(f\"  P50 Latency: {service.get('p50_latency', 0):.2f}ms\")\n    print(f\"  P95 Latency: {service.get('p95_latency', 0):.2f}ms\")\n</code></pre> <p>Response Format: <pre><code>[\n  {\n    \"service_name\": \"demo-frontend\",\n    \"span_count\": 1523,\n    \"request_rate\": 12.5,\n    \"error_rate\": 2.3,\n    \"p50_latency\": 45.2,\n    \"p95_latency\": 125.7,\n    \"p99_latency\": 250.3,\n    \"first_seen\": 1701234567890000000,\n    \"last_seen\": 1701238167890000000\n  }\n]\n</code></pre></p>"},{"location":"api/#6-get-service-dependency-map","title":"6. Get Service Dependency Map","text":"<p>Retrieve the service dependency graph:</p> cURLPython <pre><code>curl http://localhost:5005/api/service-map\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:5005/api/service-map')\ngraph = response.json()\n\nprint(\"Services:\", len(graph['nodes']))\nfor node in graph['nodes']:\n    print(f\"  - {node['service_name']} ({node['type']})\")\n\nprint(\"\\nConnections:\", len(graph['edges']))\nfor edge in graph['edges']:\n    print(f\"  {edge['source']} \u2192 {edge['target']} ({edge['call_count']} calls)\")\n</code></pre> <p>Response Format: <pre><code>{\n  \"nodes\": [\n    {\n      \"service_name\": \"demo-frontend\",\n      \"type\": \"server\",\n      \"span_count\": 1523\n    },\n    {\n      \"service_name\": \"demo-backend\",\n      \"type\": \"server\",\n      \"span_count\": 3046\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"demo-frontend\",\n      \"target\": \"demo-backend\",\n      \"call_count\": 1523\n    }\n  ]\n}\n</code></pre></p>"},{"location":"api/#7-check-system-statistics","title":"7. Check System Statistics","text":"<p>Get Redis memory usage and cardinality metrics:</p> cURLPython <pre><code>curl http://localhost:5005/api/stats\n</code></pre> <pre><code>import requests\n\nresponse = requests.get('http://localhost:5005/api/stats')\nstats = response.json()\n\nprint(f\"Total Traces: {stats.get('total_traces', 0)}\")\nprint(f\"Total Spans: {stats.get('total_spans', 0)}\")\nprint(f\"Total Logs: {stats.get('total_logs', 0)}\")\nprint(f\"Total Metrics: {stats.get('total_metrics', 0)}\")\nprint(f\"Unique Metric Names: {stats.get('unique_metric_names', 0)}\")\nprint(f\"Redis Memory: {stats.get('redis_memory_mb', 0):.2f} MB\")\n</code></pre> <p>Response Format: <pre><code>{\n  \"total_traces\": 1523,\n  \"total_spans\": 7615,\n  \"total_logs\": 15230,\n  \"total_metrics\": 45,\n  \"unique_metric_names\": 12,\n  \"redis_memory_mb\": 45.7,\n  \"cardinality_limit\": 1000,\n  \"cardinality_usage_pct\": 1.2,\n  \"uptime_seconds\": 3600\n}\n</code></pre></p>"},{"location":"api/#advanced-filtering","title":"Advanced Filtering","text":""},{"location":"api/#filter-spans-by-service","title":"Filter Spans by Service","text":"<pre><code>curl \"http://localhost:5005/api/spans?service=demo-frontend&amp;limit=100\"\n</code></pre>"},{"location":"api/#filter-logs-by-severity","title":"Filter Logs by Severity","text":"<pre><code>curl \"http://localhost:5005/api/logs?severity=ERROR&amp;limit=50\"\n</code></pre>"},{"location":"api/#time-based-queries","title":"Time-based Queries","text":"<p>All endpoints support <code>start_time</code> and <code>end_time</code> parameters (Unix nanoseconds):</p> <pre><code># Get traces from the last hour\nSTART=$(date -u -d '1 hour ago' +%s)000000000\nEND=$(date -u +%s)000000000\ncurl \"http://localhost:5005/api/traces?start_time=$START&amp;end_time=$END\"\n</code></pre>"},{"location":"api/#client-generation","title":"Client Generation","text":"<p>Generate API clients in any language using the OpenAPI spec:</p> <pre><code># Download OpenAPI spec\ncurl http://localhost:5005/openapi.json &gt; tinyolly-openapi.json\n\n# Generate Python client\nopenapi-generator-cli generate \\\n  -i tinyolly-openapi.json \\\n  -g python \\\n  -o ./tinyolly-python-client\n\n# Generate Go client\nopenapi-generator-cli generate \\\n  -i tinyolly-openapi.json \\\n  -g go \\\n  -o ./tinyolly-go-client\n\n# Generate TypeScript client\nopenapi-generator-cli generate \\\n  -i tinyolly-openapi.json \\\n  -g typescript-fetch \\\n  -o ./tinyolly-ts-client\n</code></pre>"},{"location":"api/#rate-limits","title":"Rate Limits","text":"<p>TinyOlly is designed for local development and has no rate limits. However:</p> <ul> <li>Memory limits apply based on Redis configuration (default: 256MB)</li> <li>Cardinality protection limits unique metric names to 1000 (configurable)</li> <li>TTL: All data expires after 30 minutes</li> </ul>"},{"location":"api/#authentication","title":"Authentication","text":"<p>TinyOlly is designed for local development and does not include authentication. Do not expose TinyOlly to the internet without adding authentication via a reverse proxy.</p>"},{"location":"api/#need-help","title":"Need Help?","text":"<ul> <li>View interactive examples in Swagger UI</li> <li>Open an issue on GitHub</li> <li>Read the technical architecture</li> </ul>"},{"location":"docker/","title":"Docker Deployment","text":"<p>Get TinyOlly running on Docker in minutes!</p> <p>TinyOlly UI showing distributed traces</p> <p>All examples are launched from the repo - clone it first or download the current GitHub release archive: <pre><code>git clone https://github.com/tinyolly/tinyolly\n</code></pre></p>"},{"location":"docker/#1-deploy-tinyolly-core-required","title":"1. Deploy TinyOlly Core (Required)","text":"<p>Start the observability backend (OTel Collector, TinyOlly Receiver, Redis, UI):</p> <pre><code>cd docker\n./01-start-core.sh\n</code></pre> <p>This starts: - OTel Collector: Listening on <code>localhost:4317</code> (gRPC) and <code>localhost:4318</code> (HTTP) - OpAMP Server: <code>ws://localhost:4320/v1/opamp</code> (WebSocket), <code>localhost:4321</code> (HTTP REST API) - TinyOlly UI: <code>http://localhost:5005</code> - TinyOlly OTLP Receiver and its Redis storage: OTLP observability back end and storage - Rebuilds images if code changes are detected  </p> <p>Open the UI: <code>http://localhost:5005</code> (empty until you send data)</p> <p>OpenTelemetry Collector + OpAMP Config Page: Navigate to the \"OpenTelemetry Collector + OpAMP Config\" tab in the UI to view and manage collector configurations remotely. See the OpAMP Configuration section below for setup instructions.</p> <p>Stop core services: <pre><code>./02-stop-core.sh\n</code></pre></p>"},{"location":"docker/#2-deploy-demo-apps-optional","title":"2. Deploy Demo Apps (Optional)","text":"<p>Deploy two Flask microservices with automatic traffic generation:</p> <pre><code>cd docker-demo\n./01-deploy-demo.sh\n</code></pre> <p>Wait 30 seconds. The demo apps automatically generate traffic - traces, logs, and metrics will appear in the UI!</p> <p>Stop demo apps: <pre><code>./02-cleanup-demo.sh\n</code></pre></p> <p>This leaves TinyOlly core running. To stop everything: <pre><code>cd docker\n./02-stop-core.sh\n</code></pre></p>"},{"location":"docker/#3-ai-agent-demo-with-ollama-optional","title":"3. AI Agent Demo with Ollama (Optional)","text":"<p>Deploy an AI agent demo with zero-code OpenTelemetry auto-instrumentation for GenAI:</p> <pre><code>cd docker-ai-agent-demo\n./01-deploy-ai-demo.sh\n</code></pre> <p>Note</p> <p>First run will pull the Ollama image and TinyLlama model (~1.5GB total). This may take a few minutes.</p> <p>This starts: - Ollama: Local LLM server with TinyLlama model (<code>http://localhost:11434</code>) - AI Agent: Python agent making LLM calls every 15 seconds, auto-instrumented with <code>opentelemetry-instrumentation-ollama</code></p> <p>View AI Traces: Navigate to the AI Agents tab in TinyOlly UI to see:</p> <ul> <li>Prompts and responses for each LLM call</li> <li>Token usage (input \u2193 / output \u2191) with color coding</li> <li>Latency per request</li> <li>Click any row to expand the full span JSON</li> </ul> <p>Watch agent logs: <pre><code>docker-compose logs -f ai-agent\n</code></pre></p> <p>Stop AI demo: <pre><code>./02-stop-ai-demo.sh\n</code></pre></p> <p>Cleanup (remove Ollama model volumes): <pre><code>./03-cleanup-ai-demo.sh\n</code></pre></p>"},{"location":"docker/#4-opentelemetry-demo-20-services-optional","title":"4. OpenTelemetry Demo (~20 Services - Optional)","text":"<p>Prerequisites: Clone the OpenTelemetry Demo first: <pre><code>git clone https://github.com/open-telemetry/opentelemetry-demo\ncd opentelemetry-demo\n</code></pre></p> <p>Configure: Edit <code>src/otel-collector/otelcol-config-extras.yml</code>: <pre><code>exporters:\n  otlphttp/tinyolly:\n    endpoint: http://otel-collector:4318\n\nservice:\n  pipelines:\n    traces:\n      exporters: [spanmetrics, otlphttp/tinyolly]\n</code></pre></p> <p>Deploy: <pre><code>export OTEL_COLLECTOR_HOST=host.docker.internal\ndocker compose up \\\n  --scale otel-collector=0 \\\n  --scale prometheus=0 \\\n  --scale grafana=0 \\\n  --scale jaeger=0 \\\n  --scale opensearch=0 \\\n  --force-recreate \\\n  --remove-orphans \\\n  --detach\n</code></pre></p> <p>Stop: <pre><code>docker compose down\n</code></pre></p> <p>Note</p> <p>This disables the demo's built-in collector, Jaeger, OpenSearch, Grafana, and Prometheus. All telemetry routes to Otel Collector -&gt; TinyOlly.</p>"},{"location":"docker/#5-use-tinyolly-with-your-own-apps","title":"5. Use TinyOlly with Your Own Apps","text":"<p>After deploying TinyOlly core (step 1 above), instrument your application to send telemetry:</p> <p>For apps running in Docker containers: Point your OpenTelemetry exporter to: - gRPC: <code>http://otel-collector:4317</code> - HTTP: <code>http://otel-collector:4318</code> </p> <p>For apps running on your host machine (outside Docker): Docker Desktop automatically exposes container ports to <code>localhost</code>. Point your OpenTelemetry exporter to: - gRPC: <code>http://localhost:4317</code> - HTTP: <code>http://localhost:4318</code> </p> <p>Example environment variables: <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\n</code></pre></p> <p>The Otel Collector will forward everything to TinyOlly's OTLP receiver, which process telemetry and stores it in Redis in OTEL format for the backend and UI to access.</p>"},{"location":"docker/#6-tinyolly-core-only-deployment-use-your-own-docker-opentelemetry-collector","title":"6. TinyOlly Core-Only Deployment: Use Your Own Docker OpenTelemetry Collector","text":"<p>If you already have an OpenTelemetry Collector or want to send telemetry directly to the TinyOlly Receiver, you can deploy the core components without the bundled OTel Collector.</p> <pre><code>cd docker-core-only\n./01-start-core.sh\n</code></pre> <p>This starts: - TinyOlly OTLP Receiver: Listening on <code>localhost:4343</code> (gRPC only) - OpAMP Server: <code>ws://localhost:4320/v1/opamp</code> (WebSocket), <code>localhost:4321</code> (HTTP REST API) - TinyOlly UI: <code>http://localhost:5005</code> - TinyOlly Redis: <code>localhost:6579</code></p> <p>Swap out the included Otel Collector for any distro of Otel Collector.</p> <p>Point your OpenTelemetry exporters to tinyolly-otlp-receiver:4343: i.e. <pre><code>exporters:\n  debug:\n    verbosity: detailed\n\n  otlp:\n    endpoint: \"tinyolly-otlp-receiver:4343\"\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp, spanmetrics]\n\n    metrics:\n      receivers: [otlp,spanmetrics]\n      processors: [batch]\n      exporters: [debug, otlp]\n\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp]\n</code></pre></p> <p>The Otel Collector will forward everything to TinyOlly's OTLP receiver, which process telemetry and stores it in Redis in OTEL format for the backend and UI to access.</p>"},{"location":"docker/#opamp-configuration-optional","title":"OpAMP Configuration (Optional)","text":"<p>The OpenTelemetry Collector + OpAMP Config page in the TinyOlly UI allows you to view and manage collector configurations remotely. To enable this feature, add the OpAMP extension to your collector config:</p> <pre><code>extensions:\n  opamp:\n    server:\n      ws:\n        endpoint: ws://localhost:4320/v1/opamp\n\nservice:\n  extensions: [opamp]\n</code></pre> <p>The default configuration template (located at <code>docker/otelcol-configs/config.yaml</code>) shows a complete example with OTLP receivers, OpAMP extension, batch processing, and spanmetrics connector. Your collector will connect to the OpAMP server and receive configuration updates through the TinyOlly UI.</p> <p>Stop core-only services: <pre><code>./02-stop-core.sh\n</code></pre></p>"},{"location":"docker/#building-images","title":"Building Images","text":"<p>By default, all deployment scripts pull pre-built images from Docker Hub. For building images locally or publishing to Docker Hub, see build/README.md.</p>"},{"location":"ebpf/","title":"eBPF Zero-Code Tracing Demo","text":"<p>This demo showcases OpenTelemetry eBPF Instrumentation (OBI) - automatic trace capture at the kernel level without any code changes to your application.</p>"},{"location":"ebpf/#what-is-obi","title":"What is OBI?","text":"<p>OpenTelemetry eBPF Instrumentation (formerly Grafana Beyla) uses eBPF to automatically capture HTTP/gRPC traces by inspecting system calls and network traffic at the Linux kernel level.</p> <p>Key Benefits: - Zero code changes - no SDK, no agent, no restarts - Language agnostic - works with Python, Go, Java, Node.js, Rust, C, PHP, and more - Protocol-level instrumentation - captures any HTTP/gRPC traffic</p>"},{"location":"ebpf/#quick-start","title":"Quick Start","text":""},{"location":"ebpf/#docker","title":"Docker","text":"<pre><code># Start TinyOlly core first\ncd docker\n./01-start-core.sh\n\n# Deploy eBPF demo (pulls pre-built images from Docker Hub)\ncd ../docker-demo-ebpf\n./01-deploy-ebpf-demo.sh\n</code></pre> <p>Access the UI at <code>http://localhost:5005</code></p>"},{"location":"ebpf/#kubernetes","title":"Kubernetes","text":"<pre><code># Start TinyOlly core first\nminikube start\n./k8s/02-deploy-tinyolly.sh\n\n# Deploy eBPF demo (pulls pre-built images from Docker Hub)\ncd k8s-demo-ebpf\n./02-deploy.sh\n</code></pre> <p>Run <code>minikube tunnel</code> in a separate terminal, then access the UI at <code>http://localhost:5002</code></p>"},{"location":"ebpf/#docker-hub-images","title":"Docker Hub Images","text":"<p>The eBPF demo uses pre-built images from Docker Hub: - <code>tinyolly/ebpf-frontend:latest</code> - Frontend with OTel SDK for metrics/logs - <code>tinyolly/ebpf-backend:latest</code> - Pure Flask backend (no OTel SDK)</p> <p>For local development, use the build scripts in each demo folder.</p>"},{"location":"ebpf/#whats-different-from-sdk-instrumentation","title":"What's Different from SDK Instrumentation?","text":""},{"location":"ebpf/#traces","title":"Traces","text":"Aspect SDK Instrumentation eBPF Instrumentation Span names Route names (<code>GET /hello</code>, <code>POST /api/users</code>) Generic (<code>in queue</code>, <code>CONNECT</code>, <code>HTTP</code>) Span attributes Rich application context (user IDs, request params) Network-level only (host, port, method) Distributed tracing Full trace propagation via headers Limited - eBPF sees connections, not header context Setup Code changes or auto-instrumentation wrapper Deploy eBPF agent alongside app <p>Example - SDK trace: <pre><code>{\n  \"trace_id\": \"abc123...\",\n  \"span_name\": \"GET /process-order\",\n  \"attributes\": {\n    \"http.method\": \"GET\",\n    \"http.route\": \"/process-order\",\n    \"http.status_code\": 200,\n    \"order.id\": \"12345\",\n    \"customer.id\": \"678\"\n  }\n}\n</code></pre></p> <p>Example - eBPF trace: <pre><code>{\n  \"trace_id\": \"def456...\",\n  \"span_name\": \"in queue\",\n  \"attributes\": {\n    \"net.host.name\": \"ebpf-frontend\",\n    \"net.host.port\": 5000\n  }\n}\n</code></pre></p>"},{"location":"ebpf/#logs","title":"Logs","text":"<p>With SDK instrumentation, logs include trace context (<code>trace_id</code>, <code>span_id</code>) for correlation:</p> <pre><code>{\n  \"message\": \"Processing order 12345\",\n  \"trace_id\": \"abc123...\",\n  \"span_id\": \"xyz789...\"\n}\n</code></pre> <p>With eBPF instrumentation, logs have no trace context because there's no tracing SDK to inject it:</p> <pre><code>{\n  \"message\": \"Processing order 12345\",\n  \"trace_id\": \"\",\n  \"span_id\": \"\"\n}\n</code></pre> <p>This is expected behavior - eBPF operates at the kernel level and cannot inject context into application logs.</p>"},{"location":"ebpf/#metrics","title":"Metrics","text":"<p>Metrics work the same way in both approaches - they're exported via the OTel SDK regardless of how traces are captured.</p>"},{"location":"ebpf/#components","title":"Components","text":""},{"location":"ebpf/#frontend-ebpf-frontend","title":"Frontend (<code>ebpf-frontend</code>)","text":"<ul> <li>Flask application with auto-traffic generation</li> <li>Metrics: Exported via OTel SDK (<code>OTLPMetricExporter</code>)</li> <li>Logs: Exported via OTel SDK (<code>OTLPLogExporter</code>)</li> <li>Traces: None from SDK - captured by eBPF agent</li> </ul>"},{"location":"ebpf/#backend-ebpf-backend","title":"Backend (<code>ebpf-backend</code>)","text":"<ul> <li>Pure Flask application - no OTel SDK at all</li> <li>Demonstrates that eBPF can trace completely uninstrumented apps</li> <li>Logs go to stdout only (not exported to OTel)</li> </ul>"},{"location":"ebpf/#ebpf-agent-otel-ebpf-agent","title":"eBPF Agent (<code>otel-ebpf-agent</code>)","text":"<ul> <li>Runs with <code>privileged: true</code> and <code>pid: host</code></li> <li>Monitors port 5000 for HTTP traffic</li> <li>Sends traces to OTel Collector</li> </ul>"},{"location":"ebpf/#when-to-use-ebpf-vs-sdk","title":"When to Use eBPF vs SDK","text":"<p>Use eBPF when: - You can't modify application code (legacy apps, third-party binaries) - You want basic HTTP observability with zero effort - You're instrumenting many polyglot services quickly</p> <p>Use SDK when: - You need rich application-level context in traces - You need log-trace correlation - You need custom spans for business logic - You need full distributed tracing with context propagation</p> <p>Hybrid approach (this demo): - Use eBPF for traces (zero-code) - Use SDK for metrics and logs (richer data)</p>"},{"location":"ebpf/#configuration","title":"Configuration","text":""},{"location":"ebpf/#docker_1","title":"Docker","text":"<p>The eBPF agent is configured via environment variables in <code>docker-compose.yml</code>:</p> <pre><code>otel-ebpf-agent:\n  image: docker.io/otel/ebpf-instrument:main\n  privileged: true\n  pid: host\n  environment:\n    - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317\n    - OTEL_EBPF_OPEN_PORT=5000\n  volumes:\n    - /sys/kernel/debug:/sys/kernel/debug:rw\n</code></pre>"},{"location":"ebpf/#kubernetes_1","title":"Kubernetes","text":"<p>In Kubernetes, the eBPF agent runs as a DaemonSet to instrument all pods on each node:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: otel-ebpf-agent\nspec:\n  template:\n    spec:\n      hostPID: true\n      containers:\n      - name: ebpf-agent\n        image: docker.io/otel/ebpf-instrument:main\n        securityContext:\n          privileged: true\n        env:\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"http://otel-collector:4317\"\n        - name: OTEL_EBPF_OPEN_PORT\n          value: \"5000\"\n        volumeMounts:\n        - name: sys-kernel-debug\n          mountPath: /sys/kernel/debug\n      volumes:\n      - name: sys-kernel-debug\n        hostPath:\n          path: /sys/kernel/debug\n</code></pre>"},{"location":"ebpf/#key-settings","title":"Key Settings","text":"<ul> <li><code>OTEL_EBPF_OPEN_PORT</code>: Which port to monitor (5000 = Flask default)</li> <li><code>privileged: true</code>: Required for eBPF kernel access</li> <li><code>hostPID: true</code> / <code>pid: host</code>: Required to see processes in other containers/pods</li> </ul>"},{"location":"ebpf/#troubleshooting","title":"Troubleshooting","text":"<p>No traces appearing? - Ensure TinyOlly core is running (<code>docker ps | grep otel-collector</code>) - Check eBPF agent logs: <code>docker logs otel-ebpf-instrumentation</code> - Verify the agent can access <code>/sys/kernel/debug</code></p> <p>Traces have wrong service name? - OBI discovers service names from process info - Set <code>OTEL_EBPF_SERVICE_NAME</code> for explicit naming</p> <p>eBPF agent won't start? - Requires Linux kernel 4.4+ with eBPF support - On macOS, runs inside Docker's Linux VM (should work) - Check Docker has sufficient privileges</p>"},{"location":"ebpf/#learn-more","title":"Learn More","text":"<ul> <li>OpenTelemetry eBPF Instrumentation Docs</li> <li>OBI GitHub Repository</li> <li>OBI Docker Setup Guide</li> </ul>"},{"location":"kubernetes/","title":"Kubernetes Deployment","text":"<p>Deploy TinyOlly on Kubernetes (Minikube) for local development!</p> <p>Service map showing microservices running on Kubernetes</p> <p>All examples are launched from the repo - clone it first or download the current GitHub release archive: <pre><code>git clone https://github.com/tinyolly/tinyolly\n</code></pre></p>"},{"location":"kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Minikube</li> <li>kubectl</li> </ul>"},{"location":"kubernetes/#1-deploy-tinyolly-core","title":"1. Deploy TinyOlly Core","text":"<ol> <li> <p>Start Minikube:</p> <pre><code>minikube start\n</code></pre> </li> <li> <p>Deploy TinyOlly:</p> <p>Images will be pulled from Docker Hub automatically:</p> <pre><code>./k8s/02-deploy-tinyolly.sh\n</code></pre> <p>Alternatively, you can manually apply all manifests with <code>kubectl apply -f k8s/</code></p> <p>Local Development Build (Optional)</p> <p>To build images locally for Minikube instead of pulling from Docker Hub: <pre><code>./build/local/build-core-minikube.sh\n</code></pre></p> </li> <li> <p>Access the UI:</p> <p>To access the TinyOlly UI (Service Type: LoadBalancer) on macOS with Minikube, you need to use <code>minikube tunnel</code>.</p> <p>Open a new terminal window and run:</p> <pre><code>minikube tunnel\n</code></pre> <p>You may be asked for your password. Keep this terminal open.</p> <p>Now you can access the TinyOlly UI at: http://localhost:5002</p> <p>OpenTelemetry Collector + OpAMP Config Page: Navigate to the \"OpenTelemetry Collector + OpAMP Config\" tab in the UI to view and manage collector configurations remotely. See the OpAMP Configuration section below for setup instructions.</p> </li> <li> <p>Send Telemetry from Host Apps:</p> <p>To send telemetry from applications running on your host machine (outside Kubernetes), use <code>kubectl port-forward</code> to expose the OTel Collector ports:</p> <p>Open a new terminal window and run:</p> <pre><code>kubectl port-forward service/otel-collector 4317:4317 4318:4318\n</code></pre> <p>Keep this terminal open. Now point your application's OpenTelemetry exporter to: - gRPC: <code>http://localhost:4317</code> - HTTP: <code>http://localhost:4318</code> </p> <p>Example environment variables: <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\n</code></pre></p> <p>For apps running inside the Kubernetes cluster: Use the Kubernetes service name: - gRPC: <code>http://otel-collector:4317</code> - HTTP: <code>http://otel-collector:4318</code> </p> </li> <li> <p>Clean Up:</p> <p>Use the cleanup script to remove all TinyOlly resources:</p> <pre><code>./k8s/03-cleanup.sh\n</code></pre> <p>Shut down Minikube: <pre><code>minikube stop\n</code></pre></p> <p>Minikube may be more stable if you delete it: <pre><code>minikube delete\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/#2-demo-applications-optional","title":"2. Demo Applications (Optional)","text":"<p>To see TinyOlly in action with instrumented microservices:</p> <pre><code>cd k8s-demo\n./02-deploy.sh\n</code></pre> <p>The deploy script pulls demo images from Docker Hub by default. For local development, you can build images locally when prompted.</p> <p>To clean up the demo: <pre><code>./03-cleanup.sh\n</code></pre></p> <p>The demo includes two microservices that automatically generate traffic, showcasing distributed tracing across service boundaries.</p>"},{"location":"kubernetes/#3-opentelemetry-demo-20-services-optional","title":"3. OpenTelemetry Demo (~20 Services - Optional)","text":"<p>To deploy the full OpenTelemetry Demo with ~20 microservices:</p> <p>Prerequisites: - TinyOlly must be deployed first (see Setup above) - Helm installed - Sufficient cluster resources (demo is resource-intensive)  </p> <p>Deploy: <pre><code>cd k8s-otel-demo\n./01-deploy-otel-demo-helm.sh\n</code></pre></p> <p>This deploys all OpenTelemetry Demo services configured to send telemetry to TinyOlly's collector via HTTP on port 4318. Built-in observability tools (Jaeger, Grafana, Prometheus) are disabled.</p> <p>Cleanup: <pre><code>cd k8s-otel-demo\n./02-cleanup-otel-demo-helm.sh\n</code></pre></p> <p>This removes the OpenTelemetry Demo but leaves TinyOlly running.</p>"},{"location":"kubernetes/#4-tinyolly-core-only-deployment-use-your-own-kubernetes-opentelemetry-collector","title":"4. TinyOlly Core-Only Deployment: Use Your Own Kubernetes OpenTelemetry Collector","text":"<p>To deploy TinyOlly without the bundled OTel Collector (e.g., if you have an existing collector daemonset). Includes OpAMP server for optional remote collector configuration management:</p> <ol> <li> <p>Deploy Core: <pre><code>cd k8s-core-only\n./01-deploy.sh\n</code></pre></p> </li> <li> <p>Access UI:     Run <code>minikube tunnel</code> and access <code>http://localhost:5002</code>.</p> </li> <li> <p>Cleanup: <pre><code>./02-cleanup.sh\n</code></pre></p> </li> </ol>"},{"location":"kubernetes/#use-tinyolly-with-any-opentelemetry-collector","title":"Use TinyOlly with Any OpenTelemetry Collector","text":"<p>Swap out the included Otel Collector for any distro of Otel Collector.</p> <p>Point your OpenTelemetry exporters to tinyolly-otlp-receiver:4343: i.e. <pre><code>exporters:\n  debug:\n    verbosity: detailed\n\n  otlp:\n    endpoint: \"tinyolly-otlp-receiver:4343\"\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp, spanmetrics]\n\n    metrics:\n      receivers: [otlp,spanmetrics]\n      processors: [batch]\n      exporters: [debug, otlp]\n\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp]\n</code></pre></p> <p>The Otel Collector will forward everything to TinyOlly's OTLP receiver, which process telemetry and stores it in Redis in OTEL format for the backend and UI to access.</p>"},{"location":"kubernetes/#opamp-configuration-optional","title":"OpAMP Configuration (Optional)","text":"<p>The OpenTelemetry Collector + OpAMP Config page in the TinyOlly UI allows you to view and manage collector configurations remotely. To enable this feature, add the OpAMP extension to your collector config:</p> <pre><code>extensions:\n  opamp:\n    server:\n      ws:\n        endpoint: ws://tinyolly-opamp-server:4320/v1/opamp\n\nservice:\n  extensions: [opamp]\n</code></pre> <p>The default configuration template (included as a ConfigMap in <code>k8s-core-only/tinyolly-opamp-server.yaml</code>) shows a complete example with OTLP receivers, OpAMP extension, batch processing, and spanmetrics connector. Your collector will connect to the OpAMP server and receive configuration updates through the TinyOlly UI.</p>"},{"location":"kubernetes/#building-images","title":"Building Images","text":"<p>By default, deployment scripts pull pre-built images from Docker Hub. For building images locally (Minikube) or publishing to Docker Hub, see build/README.md.</p>"},{"location":"licensing/","title":"Licensing","text":"<p>TinyOlly is released under the BSD 3-Clause License. The full license text is available in the LICENSE file. This license permits free use, modification, and redistribution for individuals, researchers, and small organizations.</p>"},{"location":"licensing/#community-and-small-scale-use","title":"Community and Small-Scale Use","text":"<p>TinyOlly may be used freely under the BSD license for:</p> <ul> <li>Individual developers and hobby projects  </li> <li>Academic and research use  </li> <li>Startups or small organizations  </li> <li>Limited internal development, tooling, experimentation, or evaluation  </li> </ul> <p>No additional agreements are required for these scenarios.</p>"},{"location":"licensing/#commercial-and-large-scale-use","title":"Commercial and Large-Scale Use","text":"<p>Organizations deploying TinyOlly in a broader or more intensive capacity are required to obtain a commercial license. This includes use cases such as:</p> <ul> <li>Organizations with approximately 50 or more employees  </li> <li>Deployments across multiple engineering teams  </li> <li>Production or mission-critical observability workflows  </li> <li>Integration into commercial software, platforms, or SaaS products  </li> <li>High-volume or distributed environments</li> </ul> <p>For commercial licensing inquiries, contact:  </p>"},{"location":"licensing/#rationale","title":"Rationale","text":"<p>This model ensures that TinyOlly remains fully open source and accessible to the community while providing a sustainable path for supporting organizations that rely on it at scale.</p>"},{"location":"metrics/","title":"Metrics &amp; Cardinality explorer","text":"<p>TinyOlly provides a powerful interface for analyzing OpenTelemetry metrics, with specific tools designed to help you understand the shape and cardinality of your telemetry data.</p>"},{"location":"metrics/#metrics-table","title":"Metrics Table","text":"<p>The metrics table offers a dense, high-information view of all ingested metrics.</p> <p></p> <p>Click the Chart button to visualize metric data over time:</p> <p></p>"},{"location":"metrics/#key-columns","title":"Key Columns","text":"<ul> <li>Name: The full OTel metric name (e.g., <code>http.server.response.size</code>).</li> <li>Unit: The unit of measurement (e.g., <code>By</code>, <code>ms</code>).</li> <li>Type: The metric type (Histogram, Sum, Gauge, etc.).</li> <li>Resources: Click to view the unique resource combinations associated with this metric.</li> <li>Cardinality: Shows the number of label dimensions vs the total number of unique time series (e.g., <code>8 labels / 185 series</code>).</li> </ul>"},{"location":"metrics/#cardinality-explorer","title":"Cardinality Explorer","text":"<p>Clicking on the blue Cardinality link for any metric opens the Cardinality Explorer. This tool is essential for understanding \"high cardinality\" issues and exploring your data's dimensions.</p>"},{"location":"metrics/#1-header-stats","title":"1. Header Stats","text":"<ul> <li>Total Series (Historic): The total number of unique time series seen for this metric since startup (persisted in Redis).</li> <li>Active Series (1h): The count of series seen in the last hour.</li> <li>Label Dimensions: The number of unique label keys (e.g., <code>http.method</code>, <code>http.status_code</code>).</li> </ul>"},{"location":"metrics/#2-label-analysis-table","title":"2. Label Analysis Table","text":"<p>This table helps you identify which labels are contributing most to your cardinality.</p> <ul> <li>Label Name: The key of the label.</li> <li>Cardinality: The number of unique values for this label.</li> <li>Values (Top 5): A preview of the most common values.<ul> <li>If there are more than 5 values, a clickable <code>...</code> link expands the list to show all values inline.</li> </ul> </li> </ul>"},{"location":"metrics/#3-raw-active-series","title":"3. Raw Active Series","text":"<p>A scrollable view of all active series in a PromQL-like syntax:</p> <pre><code>{container.id=\"...\", http.method=\"GET\", http.route=\"/api/traces\", http.status_code=\"200\", service.name=\"tinyolly-ui\"}\n{container.id=\"...\", http.method=\"GET\", http.route=\"/health\", http.status_code=\"200\", service.name=\"tinyolly-ui\"}\n</code></pre>"},{"location":"metrics/#export-actions","title":"Export Actions","text":"<p>Use the buttons in the \"Raw Active Series\" section to export data for offline analysis:</p> <ul> <li>Copy PromQL: Copies the visible series list to your clipboard.</li> <li>Download JSON: Downloads the full series object as a JSON file.</li> </ul>"},{"location":"metrics/#cardinality-protection","title":"Cardinality Protection","text":"<p>TinyOlly includes built-in protection against cardinality explosions to prevent memory exhaustion during local development.</p> <ul> <li>Hard Limit: 1000 unique metric names (configurable).</li> <li>Visual Warnings: <ul> <li>\u26a0\ufe0f Yellow: &gt; 70% capacity</li> <li>\ud83d\udd34 Red: &gt; 90% capacity</li> </ul> </li> <li>Behavior: Metrics exceeding the limit are dropped, and a system alert is triggered.</li> </ul> <p>See Cardinality Protection for more details.</p>"},{"location":"otel-collector/","title":"OpenTelemetry Collector","text":"<p>OpenTelemetry Collector configuration management via OpAMP</p> <p>TinyOlly uses the OpenTelemetry Collector as the telemetry ingestion and shipping layer. The collector receives telemetry from your applications and forwards it to TinyOlly's OTLP receiver.</p>"},{"location":"otel-collector/#opentelemetry-collector-opamp-config-page","title":"OpenTelemetry Collector + OpAMP Config Page","text":"<p>TinyOlly includes a web interface for managing OpenTelemetry Collector configurations via the OpAMP (Open Agent Management Protocol). Access this page through the \"OpenTelemetry Collector + OpAMP Config\" tab in the TinyOlly UI.</p> <p>Features: - View current configuration from connected collectors - Apply configuration changes with real-time validation - Browse configuration templates for common use cases - Monitor OpAMP server status and connected agents - Preview configuration diffs before applying</p> <p>Requirements: - Your collector must be configured with the OpAMP extension (see OpAMP Configuration below) - Collector must be connected to the OpAMP server</p>"},{"location":"otel-collector/#configuration","title":"Configuration","text":"<p>TinyOlly includes a sample collector configuration that you can customize for your needs. The configuration files are located at:</p> <ul> <li>Docker: <code>docker/otelcol-configs/config.yaml</code></li> <li>Kubernetes: <code>k8s/otel-collector-config.yaml</code></li> </ul>"},{"location":"otel-collector/#default-configuration","title":"Default Configuration","text":"<p>The default configuration includes:</p> <ul> <li>OTLP Receivers: Accepts telemetry on ports 4317 (gRPC) and 4318 (HTTP)</li> <li>OpAMP Extension: Enables remote configuration management via TinyOlly UI</li> <li>Span Metrics Connector: Automatically generates RED metrics from traces</li> <li>Batch Processor: Batches telemetry for efficient processing</li> <li>OTLP Exporter: Forwards all telemetry to TinyOlly's OTLP receiver</li> </ul>"},{"location":"otel-collector/#customization-examples","title":"Customization Examples","text":"<p>You can extend the collector configuration to add additional capabilities. The collector uses the <code>otel/opentelemetry-collector-contrib</code> image which includes:</p> <ul> <li>Receivers: OTLP, Prometheus, Jaeger, Zipkin, and many more</li> <li>Processors: Batch, Memory Limiter, Resource Detection, Tail Sampling, Filtering</li> <li>Connectors: Span Metrics, Service Graph</li> <li>Exporters: OTLP, Prometheus, Logging, and many more</li> </ul> <p>For a complete list of available components, see the OpenTelemetry Collector Contrib documentation.</p>"},{"location":"otel-collector/#applying-changes","title":"Applying Changes","text":""},{"location":"otel-collector/#docker","title":"Docker","text":"<p>After modifying <code>docker/otelcol-configs/config.yaml</code> rebuild/restart using: <pre><code>cd docker\n./01-start-core.sh\n</code></pre></p>"},{"location":"otel-collector/#kubernetes","title":"Kubernetes","text":"<p>After modifying <code>k8s/otel-collector-config.yaml</code>: rebuild/restart using: <pre><code>kubectl apply -f k8s/otel-collector-config.yaml\nkubectl rollout restart deployment/otel-collector\n</code></pre></p>"},{"location":"otel-collector/#using-your-own-collector","title":"Using Your Own Collector","text":"<p>You can use your own OpenTelemetry Collector instance instead of the one bundled with TinyOlly. This is useful if you have an existing collector setup or want to test specific collector configurations.</p> <p>To do this, deploy the Core-Only version of TinyOlly (see Docker Deployment or Kubernetes Deployment).</p> <p>Then, configure your collector's OTLP exporter to send data to the TinyOlly Receiver:</p> <ul> <li>Endpoint: <code>tinyolly-otlp-receiver:4343</code> (or <code>localhost:4343</code> from host)</li> <li>Protocol: gRPC</li> <li>TLS: Insecure (or configured as needed)</li> </ul> <p>Example Exporter Configuration: <pre><code>exporters:\n  otlp:\n    endpoint: \"tinyolly-otlp-receiver:4343\"\n    tls:\n      insecure: true\n</code></pre></p>"},{"location":"otel-collector/#opamp-configuration-optional","title":"OpAMP Configuration (Optional)","text":"<p>The OpenTelemetry Collector + OpAMP Config page in the TinyOlly UI allows you to view and manage collector configurations remotely. To enable this feature, add the OpAMP extension to your collector config:</p> <pre><code>extensions:\n  opamp:\n    server:\n      ws:\n        endpoint: ws://localhost:4320/v1/opamp\n\nservice:\n  extensions: [opamp]\n</code></pre> <p>The default configuration template (located at <code>docker/otelcol-configs/config.yaml</code>) shows a complete example with OTLP receivers, OpAMP extension, batch processing, and spanmetrics connector. Your collector will connect to the OpAMP server and receive configuration updates through the TinyOlly UI.</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get TinyOlly running in under 5 minutes!</p>"},{"location":"quickstart/#what-youll-get","title":"What You'll Get","text":"<ul> <li>TinyOlly UI at <code>http://localhost:5005</code></li> <li>OpenTelemetry Collector listening on ports 4317 (gRPC) and 4318 (HTTP)</li> <li>OpAMP Server for remote collector configuration management</li> <li>Demo microservices generating automatic telemetry</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop installed and running</li> <li>Git (to clone the repository)</li> <li>5 minutes of your time</li> </ul>"},{"location":"quickstart/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/tinyolly/tinyolly\ncd tinyolly\n</code></pre>"},{"location":"quickstart/#step-2-start-tinyolly-core","title":"Step 2: Start TinyOlly Core","text":"<pre><code>cd docker\n./01-start-core.sh\n</code></pre> <p>This pulls pre-built images from Docker Hub and starts:</p> <ul> <li>OpenTelemetry Collector (ports 4317/4318)</li> <li>OpAMP Server (ports 4320/4321)</li> <li>TinyOlly OTLP Receiver (internal)</li> <li>TinyOlly UI (port 5005)</li> <li>Redis storage (internal)</li> </ul> <p>Deployment time: ~30 seconds (pulls from Docker Hub)</p> <p>For local development: Use <code>./01-start-core-local.sh</code> to build images locally.</p>"},{"location":"quickstart/#step-3-deploy-demo-apps-optional-but-recommended","title":"Step 3: Deploy Demo Apps (Optional but Recommended)","text":"<p>In a new terminal:</p> <pre><code>cd docker-demo\n./01-deploy-demo.sh\n</code></pre> <p>This pulls demo images from Docker Hub and deploys two Flask microservices that automatically generate traffic.</p> <p>Wait 30 seconds for telemetry to appear!</p> <p>For local development: Use <code>./01-deploy-demo-local.sh</code> to build images locally.</p>"},{"location":"quickstart/#step-4-open-the-ui","title":"Step 4: Open the UI","text":"<p>Open your browser to: <code>http://localhost:5005</code></p> <p>You should see:</p> <p>Trace waterfall with correlated logs and span timing</p>"},{"location":"quickstart/#step-5-explore-the-features","title":"Step 5: Explore the Features","text":""},{"location":"quickstart/#traces-tab","title":"Traces Tab","text":"<p>View distributed traces across microservices with timing waterfall.</p> <p>Span waterfall showing request timing breakdown with correlated logs</p> <p>Click on a span to view detailed JSON data:</p> <p>Span detail view with full OpenTelemetry attributes</p>"},{"location":"quickstart/#logs-tab","title":"Logs Tab","text":"<p>Browse logs with trace/span correlation. Filter by severity (Error, Warn, Info, Debug).</p> <p>Real-time logs with trace and span correlation</p> <p>Click on a log entry to view full details:</p> <p>Log detail view with full attributes and resource info</p> <p>Filter to show only errors:</p> <p>Filtered error logs with trace correlation</p>"},{"location":"quickstart/#metrics-tab","title":"Metrics Tab","text":"<p>Visualize metrics with automatic charting.</p> <p>Time-series metrics visualization with rate charts</p>"},{"location":"quickstart/#service-catalog","title":"Service Catalog","text":"<p>View all services with RED metrics (Rate, Errors, Duration).</p> <p>Service catalog with RED metrics for all services</p>"},{"location":"quickstart/#service-map","title":"Service Map","text":"<p>Visualize service dependencies with an interactive graph.</p> <p>Interactive service dependency map with latency information</p>"},{"location":"quickstart/#opentelemetry-collector-opamp-config","title":"OpenTelemetry Collector + OpAMP Config","text":"<p>View and manage your OpenTelemetry Collector configuration remotely via the OpAMP protocol.</p> <p>OpenTelemetry Collector configuration management via OpAMP</p> <p>This page allows you to:</p> <ul> <li>View current configuration from connected collectors</li> <li>Apply configuration changes with validation and diff preview</li> <li>Use configuration templates for common scenarios (default, prometheus-remote-write, etc.)</li> <li>Check OpAMP server status and see connected collector agents</li> <li>Validate configurations before applying to catch errors early</li> </ul> <p>To use this feature, your OpenTelemetry Collector must be configured with the OpAMP extension (see Docker Deployment or OpenTelemetry Collector documentation).</p>"},{"location":"quickstart/#step-6-use-your-own-application","title":"Step 6: Use Your Own Application","text":"<p>Point your application's OpenTelemetry exporter to:</p> <p>For apps running on your host machine (outside Docker): <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\n</code></pre></p> <p>For apps running inside Docker: <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318\n</code></pre></p> <p>TinyOlly will automatically capture and display your telemetry!</p>"},{"location":"quickstart/#cleanup","title":"Cleanup","text":"<p>Stop demo apps (keeps TinyOlly running): <pre><code>cd docker-demo\n./02-cleanup-demo.sh\n</code></pre></p> <p>Stop everything: <pre><code>cd docker\n./02-stop-core.sh\n</code></pre></p>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configure your own OpenTelemetry Collector</li> <li>Explore the REST API at <code>http://localhost:5005/docs</code></li> <li>Deploy on Kubernetes</li> <li>Learn about the architecture</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#ui-shows-no-traceslogsmetrics","title":"UI shows \"No traces/logs/metrics\"","text":"<ul> <li>Wait 30 seconds after starting demo apps</li> <li>Check containers are running: <code>docker ps</code></li> <li>Check demo app logs: <code>docker compose -f docker-demo/docker-compose-demo.yml logs</code></li> </ul>"},{"location":"quickstart/#port-conflicts","title":"Port conflicts","text":"<ul> <li>TinyOlly uses ports 4317, 4318, 4320, 4321, 4343, 5005, 6579, 19291</li> <li>Stop conflicting services or modify ports in <code>docker-compose-tinyolly-core.yml</code></li> </ul>"},{"location":"quickstart/#demo-apps-not-generating-traffic","title":"Demo apps not generating traffic","text":"<ul> <li>Restart demo: <code>cd docker-demo &amp;&amp; ./02-cleanup-demo.sh &amp;&amp; ./01-deploy-demo.sh</code></li> <li>Check logs: <code>docker compose -f docker-demo/docker-compose-demo.yml logs demo-frontend</code></li> </ul> <p>For more help, open an issue on GitHub.</p>"},{"location":"technical/","title":"Technical Details","text":""},{"location":"technical/#architecture","title":"Architecture","text":""},{"location":"technical/#data-storage","title":"Data Storage","text":"<ul> <li>Format: Full OpenTelemetry (OTEL) format for traces, logs, and metrics  </li> <li>Redis: All telemetry stored with 30-minute TTL (compressed with ZSTD + msgpack)  </li> <li>Sorted Sets: Time-series data indexed by timestamp  </li> <li>Correlation: Native trace-metric-log correlation via trace/span IDs  </li> <li>Cardinality Protection: Prevents metric explosion  </li> <li>No Persistence: Data vanishes after TTL (ephemeral dev tool)  </li> </ul>"},{"location":"technical/#otlp-compatibility","title":"OTLP Compatibility","text":"<p>TinyOlly is fully OpenTelemetry-native: - Ingestion: Accepts OTLP/gRPC (primary) and OTLP/HTTP - Storage: Stores traces, logs, and metrics in full OTEL format with resources, scopes, and attributes - Correlation: Native support for trace/span ID correlation across all telemetry types - REST API: Exposes OTEL-formatted JSON for programmatic access - Control Plane: OpenTelemetry Collector OpAmp for dynamic configuration  </p>"}]}