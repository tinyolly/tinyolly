services:
  # Ollama LLM server with tinyllama model (single container)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - tinyolly-network
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        pid=$$!
        sleep 10
        ollama pull tinyllama
        wait $$pid
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q tinyllama || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 30
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '2.0'      # Limit to 2 CPU cores
          memory: 4G       # Limit to 4GB RAM
        reservations:
          cpus: '0.5'      # Reserve at least 0.5 CPU cores
          memory: 1G       # Reserve at least 1GB RAM

  # AI Agent with zero-code auto-instrumentation
  ai-agent:
    build: .
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      # Zero-code instrumentation config - ALL via env vars
      - OTEL_SERVICE_NAME=ai-agent-demo
      - OTEL_TRACES_EXPORTER=otlp
      - OTEL_METRICS_EXPORTER=none
      - OTEL_LOGS_EXPORTER=none
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_EXPORTER_OTLP_INSECURE=true
      # Application config
      - OLLAMA_HOST=http://ollama:11434
    networks:
      - tinyolly-network
    restart: unless-stopped

volumes:
  ollama-data:

networks:
  tinyolly-network:
    external: true
